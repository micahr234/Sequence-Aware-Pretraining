# Baseline experiment configuration for Sequence-Aware Pretraining

# Model Configuration
model:
  base_name: "Qwen/Qwen3-4B-Base"
  output_name: "sequence-aware-baseline"
  device: "auto"

# Dataset Configuration
dataset:
  name: "gsm8k"
  max_examples: null
  question_template: "{question}\n\n"
  answer_template: "\n\nThe answer is: {answer}"

# Training Configuration
training:
  # Training Parameters
  parameters:
    max_seq_length: 1024
    train_on_answers_only: true

  # Optimizer Settings
  optimizer:
    type: adamw_torch
    lr: 5e-4
    weight_decay: 0.0
    num_epochs: 1
    batch_size: 8
    grad_accumulation_steps: 4
    dropout: 0.0

  # Training Schedule
  schedule:
    warmup_steps: 0
    warmup_ratio: 0.0
    scheduler_type: "constant"

  # Training Control
  control:
    seed: 42
    log_every: 1

# Output Configuration
output:
  dir: "runs/baseline"
  save_every: 1000

# Evaluation Configuration
evaluation:
  dataset: "gsm8k"  # Dataset to evaluate on
  split: "test"  # Split to use for evaluation
  max_examples: 100  # Maximum examples to evaluate (None for all)
  interval_steps: 10  # Evaluate every N steps (None to disable)
  answer_regex: "\\n\\nThe answer is:\\s*(?P<answer>.+?)(?:\\n|$)"  # Regex pattern to extract answer (uses named group "answer")

# Logging Configuration
logging:
  wandb:
    project: "sequence-aware-pretraining"
    name: "baseline"

# Authentication Configuration
auth:
  env_vars:
    hf_token: "RUNPOD_HF_TOKEN"
    wandb_token: "RUNPOD_WANDB_TOKEN"

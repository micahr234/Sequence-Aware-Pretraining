# Baseline experiment configuration for Sequence-Aware Pretraining

# Model Configuration
model:
  base_name: "meta-llama/Llama-3.1-8B"
  output_name: "sequence-aware-baseline"

# Dataset Configuration
dataset:
  name: "gsm8k"
  max_examples: 100000

# Training Configuration
training:
  # Optimizer Settings
  optimizer:
    lr: 5e-4
    weight_decay: 0.1
    gamma: 0.99
    dropout: 0.1

  # Training Schedule
  schedule:
    num_epochs: 1
    batch_size: 64
    grad_accumulation_steps: 1
    warmup_steps: 0
    warmup_ratio: 0.03
    scheduler_type: "cosine"

  # Training Control
  control:
    seed: 42
    log_every: 10

# Training Parameters
training_params:
  max_seq_length: 2048

# Output Configuration
output:
  dir: "runs/sequence-aware-baseline"
  save_every: 1000

# Weights & Biases Configuration
wandb:
  project: "sequence-aware-pretraining"
  name: "baseline"
  
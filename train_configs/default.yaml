# Default configuration for Sequence-Aware Pretraining

# Model Configuration
model:
  base_name: "gpt2"
  # Base model to be trained using Discounted Log-Suffix SFT.
  # Effect: The foundation model that will be fine-tuned. Should be a causal language model
  # compatible with the task. GPT-2 provides a good balance of performance and efficiency.
  
  output_name: "sequence-aware-default"
  # Name for the fine-tuned model when pushing to Hugging Face Hub.
  # Effect: This exact name will be used for the hub repository.
  # Should be unique to avoid conflicts with existing models.
  
  

# Hardware Configuration
hardware:
  device: "auto"
  # Device to run training on.
  # Proposed by: PyTorch framework for device management.
  # Effect: "auto" automatically selects CUDA if available, otherwise CPU. Can be explicitly
  # set to "cuda", "cpu", or specific device IDs like "cuda:0".
  # Reference: https://pytorch.org/docs/stable/tensor_attributes.html#torch.device

# Dataset Configuration
dataset:
  name: "c4"
  # Dataset to use for training.
  # Effect: C4 (Clean Common Crawl) is excellent for broad pretraining with diverse web text.
  # Provides high-quality, diverse text for general language understanding.
  
  max_examples: 1000
  # Maximum number of examples to use from the dataset.
  # Effect: Limits the dataset size for faster experimentation. Set to null to use all examples.
  # Useful for debugging and quick experiments with smaller subsets.

# Training Configuration
training:
  # Optimizer Settings
  optimizer:
    lr: 1e-4
    # Learning rate for the optimizer.
    # Effect: Standard learning rate for pretraining. Higher than fine-tuning rates.
    # Typical range for pretraining: 1e-4 to 5e-4.
    
    weight_decay: 0.1
    # L2 regularization coefficient for the optimizer.
    # Effect: Standard weight decay for pretraining. Higher than fine-tuning values.
    # Typical range for pretraining: 0.1 to 0.2.
    
    gamma: 0.99
    # Discount factor for position-based weighting in discounted log-suffix training.
    # Effect: Higher gamma for pretraining to emphasize later tokens more strongly.
    # Typical range for pretraining: 0.95 to 0.99.
    
    dropout: 0.1
    # Dropout rate for regularization during training.
    # Effect: Standard dropout rate for pretraining. Prevents overfitting.
    # Typical range for pretraining: 0.1 to 0.2.

  # Training Schedule
  schedule:
    num_epochs: 1
    # Number of complete passes through the training dataset.
    # Effect: Standard for pretraining. More epochs than fine-tuning.
    # Typical range for pretraining: 1-5 epochs.
    
    batch_size: 16
    # Number of examples processed in each training step.
    # Effect: Larger batch size for pretraining. More stable gradients.
    # Typical range for pretraining: 4-16.
    
    grad_accumulation_steps: 4
    # Number of gradient accumulation steps before updating parameters.
    # Effect: Simulates larger batch sizes. Effective batch size = batch_size Ã— grad_accumulation_steps.
    # Typical range for pretraining: 2-8.
    
    warmup_steps: 0
    # Number of warmup steps for learning rate scheduling.
    # Effect: Standard for pretraining. Gradual learning rate increase.
    # Typical range for pretraining: 0-1000 steps.
    
    warmup_ratio: 0.03
    # Warmup ratio as a fraction of total training steps.
    # Effect: Standard warmup ratio for pretraining. 3% of total steps.
    # Typical range for pretraining: 0.01 to 0.05.
    
    scheduler_type: "cosine"  # Options: "linear", "cosine", "constant", "polynomial"
    # Type of learning rate scheduler to use.
    # Effect: Cosine annealing is standard for pretraining. Smooth decay.
    # Typical for pretraining: "cosine" or "linear".

    

  # Training Control
  control:
    seed: 42
    # Random seed for reproducibility.
    # Proposed by: Standard practice in scientific computing for reproducibility.
    # Effect: Ensures reproducible results by fixing random number generation.
    # Critical for scientific experiments and debugging.
    # Reference: https://en.wikipedia.org/wiki/Random_seed
    
    log_every: 10
    # Frequency of logging training metrics.
    # Proposed by: Standard practice in ML training monitoring.
    # Effect: Controls how often training progress is logged. More frequent logging
    # provides better monitoring but increases I/O overhead.
    # Reference: Standard ML training practice

# Training Parameters
training_params:
  max_seq_length: 1024
  # Maximum length of input sequences.
  # Effect: Standard length for pretraining. Longer sequences for better context.
  # Typical range for pretraining: 512-2048.

# Output Configuration
output:
  dir: "runs/default"
  # Directory to save model checkpoints and outputs.
  # Proposed by: Standard practice in ML training for output management.
  # Effect: All training outputs (checkpoints, logs, etc.) will be saved to this directory.
  # Should be unique for each experiment to avoid conflicts.
  # Reference: Standard ML training practice
  
  save_every: 1000
  # Frequency of model checkpoint saving.
  # Proposed by: Standard practice in ML training for checkpoint management.
  # Effect: Saves model checkpoints every N steps. More frequent saving provides
  # better recovery options but increases I/O overhead and disk usage.
  # Reference: Standard ML training practice

# Weights & Biases Configuration
wandb:
  project: "sequence-aware-pretraining"
  # Weights & Biases project name for experiment tracking.
  # Proposed by: Weights & Biases for ML experiment management.
  # Effect: Groups related experiments together in the W&B dashboard.
  # Should be consistent across related experiments.
  # Reference: https://docs.wandb.ai/
  
  name: "default"
  # Weights & Biases run name for this specific experiment.
  # Proposed by: Weights & Biases for ML experiment management.
  # Effect: Identifies this specific run in the W&B dashboard. Should be unique
  # for each experiment to avoid confusion.
  # Reference: https://docs.wandb.ai/

# Environment Variable Names for Authentication
env_vars:
  hf_token: "RUNPOD_HF_TOKEN"      # Hugging Face token environment variable name
  # Environment variable name containing the Hugging Face authentication token.
  
  wandb_token: "RUNPOD_WANDB_TOKEN"  # Weights & Biases token environment variable name
  # Environment variable name containing the Weights & Biases API key.